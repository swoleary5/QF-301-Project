{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-04T07:27:34.503508900Z",
     "start_time": "2023-12-04T07:27:31.931436500Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import Necessary Libraries\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, learning_curve\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "from yahoo_fin.stock_info import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import learning_curve\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta, date\n",
    "import yfinance as yf\n",
    "import pandas_market_calendars as mcal\n",
    "from gekko import GEKKO\n",
    "\n",
    "# Create a calendar for NYSE\n",
    "nyse = mcal.get_calendar('NYSE')\n",
    "\n",
    "# Ignore warnings from program\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define functions to gather the indicators for each stock/pair\n",
    "\"\"\"\n",
    "def normalize(series):\n",
    "    series = pd.Series(series)\n",
    "    \n",
    "    # Get the minimum and maximum values of the series.\n",
    "    series.fillna(0, inplace = True)\n",
    "\n",
    "    # Normalize the series by subtracting the minimum value and dividing by the range.\n",
    "    return (series - series.mean()) / series.std()\n",
    "\n",
    "\n",
    "# First indicator is the RSI, this is a continuous score, works for time series\n",
    "def getRSI(stock, window):\n",
    "    # First we must calculate the price differences day over day\n",
    "    diff = stock.diff()\n",
    "\n",
    "    # Now we find the gains and losses using clip()\n",
    "    winners = diff.clip(lower=0)\n",
    "    losers = diff.clip(upper=0).abs()\n",
    "\n",
    "    # Calculate mean of winners and losers using simple moving average\n",
    "    mean_win = winners.rolling(window).mean()\n",
    "    mean_loss = losers.rolling(window).mean().abs()\n",
    "\n",
    "    # Calculate RS and return RSI\n",
    "    rs = mean_win / mean_loss\n",
    "    return 100 - (100 / (1 + rs))\n",
    "\n",
    "\n",
    "# Get the continuous Volume Weighted Average Price\n",
    "def getVWAP(stock):\n",
    "    # Get all historical values of the stock's volume\n",
    "    quant = stock[\"volume\"].values\n",
    "\n",
    "    # Get all historical values of average price of stock\n",
    "    price = ((stock[\"close\"] + stock[\"high\"] + stock[\"low\"]) / 3).values\n",
    "\n",
    "    # Calculate VWAP\n",
    "    vwap = (price * quant).cumsum() / quant.cumsum()\n",
    "\n",
    "    # Return VWAP\n",
    "    return vwap\n",
    "\n",
    "\n",
    "# This function calculates the simple moving average of a given time frame\n",
    "def calc_avg(changes, window_size):\n",
    "    windows = changes.rolling(window_size)\n",
    "    moving_avg = windows.mean()\n",
    "    moving_avg_lst = moving_avg.values.tolist()\n",
    "    final_list = moving_avg_lst[window_size - 1:]\n",
    "    final_index = changes.index[window_size - 1:]\n",
    "    return pd.Series(final_list, final_index)\n",
    "\n",
    "\n",
    "def sharpe(stock):\n",
    "    # Begin with 5-year t-bill rate from 2/16/2018, the start of our portfolio, yearly expected return\n",
    "    rfr = 2.67 * (1 / np.sqrt(5))\n",
    "\n",
    "    # to calculate sharpe ratio, we follow the formula (return - rfr) / standard deviation\n",
    "    return round(((np.mean(stock) - rfr) / np.std(stock)) * np.sqrt(5), 2)\n",
    "\n",
    "def downside_deviation(series, threshold):\n",
    "    # Calculate returns below the threshold\n",
    "    below_threshold = series[series < threshold]\n",
    "    \n",
    "    # Calculate squared deviations from the threshold\n",
    "    squared_deviations = (below_threshold - threshold) ** 2\n",
    "    \n",
    "    # Calculate the mean of squared deviations\n",
    "    mean_squared_deviations = squared_deviations.mean()\n",
    "    \n",
    "    # Calculate the square root to get the downside deviation\n",
    "    downside_dev = np.sqrt(mean_squared_deviations)\n",
    "    \n",
    "    return downside_dev\n",
    "\n",
    "# This is a function that will track the mean spread of the two stocks over time, this should hopefully help our algorithm a lot in finding entry and exit points\n",
    "\n",
    "def cumulative_mean(lst):\n",
    "    result = []\n",
    "    for i in range(1, len(lst) + 1):\n",
    "        subset_list = lst[:i]\n",
    "        mean_value = sum(subset_list) / len(subset_list)\n",
    "        result.append(mean_value)\n",
    "    return result\n",
    "\n",
    "def cumulative_vol(lst):\n",
    "    result = []\n",
    "    for i in range(1, len(lst) + 1):\n",
    "        subset_list = lst[:i]\n",
    "        std_value = np.std(subset_list)\n",
    "        result.append(std_value)\n",
    "    return result"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T07:27:36.152379400Z",
     "start_time": "2023-12-04T07:27:36.122856100Z"
    }
   },
   "id": "3dc8f91747e63a90"
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [],
   "source": [
    "# Define the start and end dates of our data collection using datetime\n",
    "# These are constant variables, adjust them here if necessary\n",
    "start_date = date(year=2010, month=1, day=1)\n",
    "end_date = date(year=2023, month=12, day=3)\n",
    "\n",
    "# Specify the split point (e.g., a specific date) as a datetime object\n",
    "split_date = pd.to_datetime(date(year=2023, month=8, day=1))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T07:45:21.805580900Z",
     "start_time": "2023-12-04T07:45:21.779224500Z"
    }
   },
   "id": "e4a4bef2c4848798"
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "outputs": [],
   "source": [
    "def construct(stock, n_days):\n",
    "    # Get the data from each given stock from yahoo_fin\n",
    "    try:\n",
    "        stock_data = get_data(stock.strip(), start_date, end_date)\n",
    "        SPY = get_data(\"SPY\", start_date, end_date)\n",
    "    except AssertionError as e:\n",
    "        print(f\"Error fetching data for pair {stock1}, {stock2}: {e}\")\n",
    "        return 0\n",
    "    \n",
    "    # Ensure the index is a DatetimeIndex\n",
    "    if not isinstance(stock_data.index, pd.DatetimeIndex):\n",
    "        stock_data['Date'] = pd.to_datetime(stock_data['date'])\n",
    "        stock_data.set_index('Date', inplace=True)\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    df[\"Stock\"] = stock_data[\"adjclose\"]\n",
    "    df[\"High\"] = normalize(stock_data[\"high\"])\n",
    "    df[\"Low\"] = normalize(stock_data[\"low\"])\n",
    "    df[\"Open\"] = normalize(stock_data[\"open\"])\n",
    "    df[\"volume\"] = normalize(stock_data[\"volume\"])\n",
    "    df[\"High v Low\"] = normalize(stock_data[\"high\"] - stock_data[\"low\"])\n",
    "    df[\"Close v Open\"] = normalize(stock_data[\"adjclose\"] - stock_data[\"open\"])\n",
    "    df[\"Daily LGReturn\"] = np.log1p(stock_data[\"adjclose\"].pct_change())\n",
    "    df[\"N_days LGReturn\"] = np.log1p(stock_data[\"adjclose\"].pct_change(n_days))\n",
    "    df[\"RD2\"] = df[\"N_days LGReturn\"].diff()\n",
    "    df[\"RD3\"] = df[\"RD2\"].diff()\n",
    "    df[\"10 MA\"] = calc_avg(df[\"Stock\"], 10)\n",
    "    df[\"20 MA\"] = calc_avg(df[\"Stock\"], 20)\n",
    "    df[\"50 MA\"] = calc_avg(df[\"Stock\"], 50)\n",
    "    df[\"100 MA\"] = calc_avg(df[\"Stock\"], 100)\n",
    "    df[\"200 MA\"] = calc_avg(df[\"Stock\"], 200)\n",
    "    df[\"10 MV\"] = calc_avg(df[\"volume\"], 10)\n",
    "    df[\"20 MV\"] = calc_avg(df[\"volume\"], 20)\n",
    "    df[\"50 MV\"] = calc_avg(df[\"volume\"], 50)\n",
    "    df[\"100 MV\"] = calc_avg(df[\"volume\"], 100)\n",
    "    df[\"200 MV\"] = calc_avg(df[\"volume\"], 200)\n",
    "    df[\"10D volatility\"] = normalize(df[\"Stock\"].rolling(10).std()*(252**0.5))\n",
    "    df[\"20D volatility\"] = normalize(df[\"Stock\"].rolling(20).std()*(252**0.5))\n",
    "    df[\"50D volatility\"] = normalize(df[\"Stock\"].rolling(50).std()*(252**0.5))\n",
    "    df[\"100D volatility\"] = normalize(df[\"Stock\"].rolling(100).std()*(252**0.5))\n",
    "    df[\"200D volatility\"] = normalize(df[\"Stock\"].rolling(200).std()*(252**0.5))\n",
    "    df[\"RSI\"] = getRSI(stock_data[\"adjclose\"], 14)\n",
    "    df[\"RSI2\"] = getRSI(stock_data[\"adjclose\"], n_days)\n",
    "    \n",
    "    df[\"SPY\"] = SPY[\"adjclose\"]\n",
    "    df[\"SPY High\"] = normalize(SPY[\"high\"])\n",
    "    df[\"SPY Low\"] = normalize(SPY[\"low\"])\n",
    "    df[\"SPY Open\"] = normalize(SPY[\"open\"])\n",
    "    df[\"SPY volume\"] = normalize(SPY[\"volume\"])\n",
    "    df[\"SPY High v Low\"] = normalize(SPY[\"high\"] - SPY[\"low\"])\n",
    "    df[\"SPY Close v Open\"] = normalize(SPY[\"adjclose\"] - SPY[\"open\"])\n",
    "    df[\"SPY Daily LGReturn\"] = np.log1p(SPY[\"adjclose\"].pct_change())\n",
    "    df[\"SPY N_days LGReturn\"] = np.log1p(SPY[\"adjclose\"].pct_change(n_days))\n",
    "    df[\"SPY RD2\"] = df[\"SPY N_days LGReturn\"].diff()\n",
    "    df[\"SPY RD3\"] = df[\"SPY RD2\"].diff()\n",
    "    df[\"SPY 10 MA\"] = calc_avg(df[\"SPY\"], 10)\n",
    "    df[\"SPY 20 MA\"] = calc_avg(df[\"SPY\"], 20)\n",
    "    df[\"SPY 50 MA\"] = calc_avg(df[\"SPY\"], 50)\n",
    "    df[\"SPY 100 MA\"] = calc_avg(df[\"SPY\"], 100)\n",
    "    df[\"SPY 200 MA\"] = calc_avg(df[\"SPY\"], 200)\n",
    "    df[\"SPY 10 MV\"] = calc_avg(df[\"SPY volume\"], 10)\n",
    "    df[\"SPY 20 MV\"] = calc_avg(df[\"SPY volume\"], 20)\n",
    "    df[\"SPY 50 MV\"] = calc_avg(df[\"SPY volume\"], 50)\n",
    "    df[\"SPY 100 MV\"] = calc_avg(df[\"SPY volume\"], 100)\n",
    "    df[\"SPY 200 MV\"] = calc_avg(df[\"SPY volume\"], 200)\n",
    "    df[\"SPY 10D volatility\"] = normalize(df[\"SPY\"].rolling(10).std()*(252**0.5))\n",
    "    df[\"SPY 20D volatility\"] = normalize(df[\"SPY\"].rolling(20).std()*(252**0.5))\n",
    "    df[\"SPY 50D volatility\"] = normalize(df[\"SPY\"].rolling(50).std()*(252**0.5))\n",
    "    df[\"SPY 100D volatility\"] = normalize(df[\"SPY\"].rolling(100).std()*(252**0.5))\n",
    "    df[\"SPY 200D volatility\"] = normalize(df[\"SPY\"].rolling(200).std()*(252**0.5))\n",
    "    df[\"SPY RSI\"] = getRSI(SPY[\"adjclose\"], 14)\n",
    "    df[\"SPY RSI2\"] = getRSI(SPY[\"adjclose\"], n_days)\n",
    "    \n",
    "    df = df.iloc[::n_days]\n",
    "    \n",
    "    df[\"Stock Lag 1\"] = df[\"Stock\"].shift(1)\n",
    "    df[\"Stock Lag 2\"] = df[\"Stock\"].shift(2)\n",
    "    df[\"Stock Lag 3\"] = df[\"Stock\"].shift(3)\n",
    "    df[\"Return Lag 1\"] = df[\"N_days LGReturn\"].shift(1)\n",
    "    df[\"Return Lag 2\"] = df[\"N_days LGReturn\"].shift(2)\n",
    "    df[\"Return Lag 3\"] = df[\"N_days LGReturn\"].shift(3)\n",
    "    \n",
    "    df[\"SPY Lag 1\"] = df[\"SPY\"].shift(1)\n",
    "    df[\"SPY Lag 2\"] = df[\"SPY\"].shift(2)\n",
    "    df[\"SPY Lag 3\"] = df[\"SPY\"].shift(3)\n",
    "    df[\"SPY Return Lag 1\"] = df[\"SPY N_days LGReturn\"].shift(1)\n",
    "    df[\"SPY Return Lag 2\"] = df[\"SPY N_days LGReturn\"].shift(2)\n",
    "    df[\"SPY Return Lag 3\"] = df[\"SPY N_days LGReturn\"].shift(3)\n",
    "    \n",
    "    df[\"return spread\"] = (df[\"N_days LGReturn\"] - df[\"SPY N_days LGReturn\"])\n",
    "    df[\"RS-Mean\"] = cumulative_mean(df[\"return spread\"])\n",
    "    df[\"Distance\"] = df[\"RS-Mean\"] - df[\"return spread\"]\n",
    "    df[\"DD1\"] = df[\"Distance\"].diff()\n",
    "    df[\"DD2\"] = df[\"DD1\"].diff()\n",
    "    df[\"DD3\"] = df[\"DD2\"].diff()\n",
    "    df[\"SD\"] = df[\"return spread\"].diff()  # First derivative of spread, change in spread\n",
    "    df[\"S2D\"] = df[\"SD\"].diff()  # Second Derivative, concavity of spread\n",
    "    df[\"S3D\"] = df[\"S2D\"].diff()  # Third Derivative, concavity of change in spread\n",
    "    \n",
    "    # Return the dataframe - NaN values from first day\n",
    "    return df[1:]\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T07:45:26.367988600Z",
     "start_time": "2023-12-04T07:45:26.359990300Z"
    }
   },
   "id": "22ccf95b5d20cd35"
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [],
   "source": [
    "# Create function to split train and test\n",
    "def split_data(df, split_date):\n",
    "    # Add next day spread to dataframe and remove\n",
    "    df[\"y\"] = df[\"return spread\"].shift(-1)\n",
    "    df = df[:-1]\n",
    "\n",
    "    # Filter the data into training and testing sets\n",
    "    train_data = df[df.index < split_date]\n",
    "    test_data = df[df.index >= split_date]\n",
    "    \n",
    "    test_data = test_data.reset_index(drop = True)\n",
    "    \n",
    "    test_data = test_data.iloc[:1]\n",
    "\n",
    "    # Separate the features (X) and target (y) for both sets\n",
    "    y_train = train_data[\"y\"]\n",
    "    X_train = train_data.drop(columns=[\"y\"])\n",
    "\n",
    "    y_test = test_data[\"y\"]\n",
    "    X_test = test_data.drop(columns=[\"y\"])\n",
    "    \n",
    "    \n",
    "    return X_train, y_train, X_test, y_test"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T07:45:26.677802200Z",
     "start_time": "2023-12-04T07:45:26.652772900Z"
    }
   },
   "id": "1483f62d4b6691b8"
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [],
   "source": [
    "def fake_split(df, split_date):\n",
    "    test_data = df[df.index >= split_date]\n",
    "    \n",
    "    return len(test_data)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T07:45:26.867342200Z",
     "start_time": "2023-12-04T07:45:26.833695700Z"
    }
   },
   "id": "66c26033ceeb4bb6"
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "outputs": [],
   "source": [
    "def train_mod(stock, split_date):\n",
    "    # Define a dictionary with the hyperparameter grid\n",
    "    X_train, y_train, X_test, y_test = split_data(stock, split_date)\n",
    "    \n",
    "    \"\"\"\n",
    "    param_grid = {\n",
    "        'max_depth': [100],  # Max depth of trees\n",
    "        'learning_rate': [.5],\n",
    "        'n_estimators': [1000],\n",
    "        'subsample': [1],\n",
    "        'colsample_bytree': [1],\n",
    "        'colsample_bylevel': [1],\n",
    "        'min_child_weight': [14]\n",
    "    }\n",
    "    \"\"\"\n",
    "    param_grid = {\n",
    "        'max_depth': list(range(10, 1001, 20)),  # Max depth of trees\n",
    "        'learning_rate': [i / 100 for i in range(1, 100, 5)],\n",
    "        'n_estimators': list(range(10, 1001, 20)),\n",
    "        'subsample': [i / 100 for i in range(60, 101, 5)],\n",
    "        'colsample_bytree': [i / 100 for i in range(60, 101, 5)],\n",
    "        'colsample_bylevel': [i / 100 for i in range(60, 101, 5)],\n",
    "        'min_child_weight': list(range(0, 51, 5))\n",
    "    }\n",
    "    #\"\"\"\n",
    "    # Create an XGBoost regressor model, uses squared error as the error parameter\n",
    "    model = xgb.XGBRegressor(objective=\"reg:squarederror\", \n",
    "                             device=\"cuda\", \n",
    "                             validate_parameters=True, \n",
    "                             early_stopping_rounds=20,\n",
    "                             tree_method = 'exact')\n",
    "    \n",
    "    # Create a GridSearchCV object\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=model,\n",
    "        param_distributions=param_grid,  # Use param_grid instead of param_distributions\n",
    "        scoring=\"neg_mean_squared_error\",  # Choose an appropriate scoring metric\n",
    "        cv=5,  # Number of cross-validation folds\n",
    "        verbose=0,\n",
    "        n_jobs=-1,  # Use all available CPU cores\n",
    "        n_iter=25\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        random_search.fit(X_train, y_train, \n",
    "                        eval_set=[(X_train, y_train)], \n",
    "                        verbose=False\n",
    "                        )\n",
    "    except ValueError:\n",
    "        return 0\n",
    "\n",
    "    # Get the best hyperparameters and model\n",
    "    best_params = random_search.best_params_\n",
    "    best_model = random_search.best_estimator_\n",
    "    best_model.save_model(\"301-model.json\")\n",
    "    \n",
    "    # Make predictions with the best model\n",
    "    prediction = best_model.predict(X_test)\n",
    "    \n",
    "    #xgb.plot_importance(best_model)\n",
    "    #plt.show()\n",
    "    \n",
    "    # Calculate the mean squared error\n",
    "    mse = mean_squared_error(y_test, prediction)\n",
    "    \n",
    "    error = mse ** 0.5\n",
    "    \n",
    "    mape = np.mean(np.abs((y_test - prediction) / y_test)) * 100\n",
    "    \n",
    "    #plt.plot(y_test.index, normalize(prediction), label = \"predictions\")\n",
    "    #plt.plot(y_test.index, normalize(y_test), label = \"actual\")\n",
    "    #plt.legend()\n",
    "    #plt.show()\n",
    "    \n",
    "    # Print the best hyperparameters and MSE\n",
    "    #print(\"Best Hyperparameters:\", best_params)\n",
    "    #print(\"Standard Error:\", mse ** 0.5)\n",
    "    \n",
    "    return prediction, y_test, y_train"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T07:45:27.054341600Z",
     "start_time": "2023-12-04T07:45:27.019176800Z"
    }
   },
   "id": "b3319edd1c825e10"
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "outputs": [],
   "source": [
    "basket = [\"VGT\", \"VNQ\", \"XLF\", \"XLV\", \"XLE\", \"XLI\", \"XLY\", \"XLP\", \"XLU\", \"XLB\", \"VOX\", \"GLD\", \"SPAB\"]\n",
    "top5 =[\"AAPL\", \"MSFT\", \"GOOG\", \"AMZN\", \"NVDA\"]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T07:45:27.210256600Z",
     "start_time": "2023-12-04T07:45:27.180681700Z"
    }
   },
   "id": "ccaa8092b53fcecb"
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "outputs": [],
   "source": [
    "def get_expected_rtrn(predicted_rtrns, i):\n",
    "    return predicted_rtrns[i][len(predicted_rtrns) - 1]\n",
    "\n",
    "def get_covariance(cov, i, j):\n",
    "    return cov[i][j]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T07:45:27.663111900Z",
     "start_time": "2023-12-04T07:45:27.638097Z"
    }
   },
   "id": "6c546a09e9ce497b"
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "outputs": [],
   "source": [
    "def find_max_sharpe_ratio(pareto_frontier, risk_free_rate=0.005):\n",
    "    max_sharpe_ratio = -1\n",
    "    optimal_weights = None\n",
    "\n",
    "    for point in pareto_frontier:\n",
    "        portfolio_return, portfolio_risk, asset_weights = point\n",
    "        if portfolio_risk != 0:  # To avoid division by zero\n",
    "            # Calculate Sharpe Ratio\n",
    "            sharpe_ratio = (portfolio_return - risk_free_rate) / abs(portfolio_risk)\n",
    "            if sharpe_ratio > max_sharpe_ratio:\n",
    "                max_sharpe_ratio = sharpe_ratio\n",
    "                optimal_weights = asset_weights\n",
    "\n",
    "    return optimal_weights"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T07:45:27.943388800Z",
     "start_time": "2023-12-04T07:45:27.917405200Z"
    }
   },
   "id": "71f0089d86f0b61c"
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "outputs": [],
   "source": [
    "def get_predictions(assets, split_date):\n",
    "    \n",
    "    pred_rtrns = pd.DataFrame()\n",
    "    \n",
    "    for i, item in enumerate(assets):\n",
    "        print(i * \".\")\n",
    "        df = construct(item, 20)\n",
    "    \n",
    "        prediction, actual, y_train = train_mod(df, split_date)\n",
    "    \n",
    "        pred_rtrns[item] = pd.concat([y_train, pd.Series(prediction[0])])\n",
    "        \n",
    "    pred_rtrns = pred_rtrns.reset_index(drop = True)\n",
    "        \n",
    "    return pred_rtrns      "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T07:45:28.251041Z",
     "start_time": "2023-12-04T07:45:28.216626900Z"
    }
   },
   "id": "144e05e0e1efa501"
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "outputs": [
    {
     "data": {
      "text/plain": "'\\npareto_frontier = construct_pareto_frontier([\"AAPL\", \"MSFT\"])\\n\\nreturns = [point[0] for point in pareto_frontier]\\nrisks = [abs(point[1]) for point in pareto_frontier]\\n\\n# Plotting the Pareto frontier\\nplt.figure(figsize=(10, 6))\\nplt.scatter(risks, returns, c=\\'blue\\', marker=\\'o\\')  # Scatter plot\\n# plt.plot(risks, returns, \\'b-\\')  # Use this for a line plot\\n\\n# Customizing the plot\\nplt.title(\\'Pareto Frontier - Portfolio Optimization\\')\\nplt.xlabel(\\'Risk\\')\\nplt.ylabel(\\'Return\\')\\nplt.grid(True)\\n\\n# Show the plot\\nplt.show()\\n'"
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def construct_pareto_frontier(assets, split_date):\n",
    "    \n",
    "    predicted_rtrns = get_predictions(assets, split_date)\n",
    "    \n",
    "    covariance_matrix = predicted_rtrns.cov()\n",
    "    \n",
    "    # Initialize GEKKO model\n",
    "    m = GEKKO(remote=False)\n",
    "   \n",
    "    w1 = m.Param(value=1.0)\n",
    "    w2 = m.Param(value=1.0 - w1)\n",
    "    \n",
    "    # Variables\n",
    "    X = {asset: m.Var(lb=0, ub=1, name=asset) for asset in assets}\n",
    "    \n",
    "    # Parameters for expected returns and covariances\n",
    "    r = {asset: get_expected_rtrn(predicted_rtrns, asset) for asset in assets}\n",
    "   \n",
    "    cov_params = {(i, j): get_covariance(covariance_matrix, i, j) for i in assets for j in assets}\n",
    "    \n",
    "    # Weights for objectives as GEKKO parameters\n",
    "    \n",
    "    # Constraint: Sum of X values equal to 1\n",
    "    m.Equation(sum([X[asset] for asset in assets]) == 1)\n",
    "    \n",
    "    # Objective functions\n",
    "    obj1 = sum([X[asset] * r[asset] for asset in assets])\n",
    "    obj2 = - sum([X[i] * X[j] * cov_params[i, j] for i in assets for j in assets])\n",
    "    \n",
    "    # Combined Objective\n",
    "    m.Maximize(w1 * obj1 + w2 * obj2)\n",
    "    \n",
    "    # Solve the optimization problem\n",
    "    m.solve(disp=False)\n",
    "    \n",
    "    w_list = (np.logspace(0, 1.0, 51, base = 10) - 1) / 10\n",
    "    \n",
    "    pareto_frontier = []\n",
    "\n",
    "    # Iterate over a set of weights\n",
    "    for w in w_list:\n",
    "        # Set the weights for the objectives\n",
    "        w1.value = w\n",
    "        w2.value = 1 - w\n",
    "        \n",
    "        # Solve the model\n",
    "        m.solve(disp=False)\n",
    "\n",
    "        # Calculate return and risk\n",
    "        portfolio_return = sum([X[asset].value[0] * r[asset] for asset in assets])\n",
    "        portfolio_risk = - sum([X[i].value[0] * X[j].value[0] * cov_params[i, j] for i in assets for j in assets])\n",
    "\n",
    "        # Collect the weights\n",
    "        asset_weights = [X[asset].value[0] for asset in assets]\n",
    "\n",
    "        # Add to the Pareto frontier\n",
    "        pareto_frontier.append((portfolio_return, portfolio_risk, asset_weights))\n",
    "\n",
    "    return pareto_frontier\n",
    "\n",
    "\"\"\"\n",
    "pareto_frontier = construct_pareto_frontier([\"AAPL\", \"MSFT\"])\n",
    "\n",
    "returns = [point[0] for point in pareto_frontier]\n",
    "risks = [abs(point[1]) for point in pareto_frontier]\n",
    "\n",
    "# Plotting the Pareto frontier\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(risks, returns, c='blue', marker='o')  # Scatter plot\n",
    "# plt.plot(risks, returns, 'b-')  # Use this for a line plot\n",
    "\n",
    "# Customizing the plot\n",
    "plt.title('Pareto Frontier - Portfolio Optimization')\n",
    "plt.xlabel('Risk')\n",
    "plt.ylabel('Return')\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T07:45:28.561774700Z",
     "start_time": "2023-12-04T07:45:28.522512800Z"
    }
   },
   "id": "b4134dc33851e822"
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "outputs": [],
   "source": [
    "def period_rtrn(assets, capital, allocations, n_days, length):\n",
    "    \n",
    "    rtrns = []\n",
    "    \n",
    "    for i, item in enumerate(assets):\n",
    "        df = construct(item, n_days)\n",
    "        \n",
    "        df = df.reset_index(drop = True)\n",
    "        \n",
    "        rtrn = (capital * allocations[i]) * (((df[\"Stock\"][len(df[\"Stock\"])- length]) /\n",
    "                                                 (df[\"Stock\"][len(df[\"Stock\"]) - length - 1])) - 1)\n",
    "        \n",
    "        rtrns.append(rtrn)\n",
    "\n",
    "    return capital + sum(rtrns)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T07:45:28.869773800Z",
     "start_time": "2023-12-04T07:45:28.855111200Z"
    }
   },
   "id": "b9ce717b7dcc3552"
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "outputs": [],
   "source": [
    "def backtest(assets, split_date, capital, test, n_days):\n",
    "    length = test\n",
    "    cap = [capital]\n",
    "    k = 0\n",
    "\n",
    "    while length > 1:\n",
    "        print(k, \"Months, Capital: $\", round(cap[-1], 4))\n",
    "        \n",
    "        allocations = find_max_sharpe_ratio(construct_pareto_frontier(assets, split_date), 0.005)\n",
    "        \n",
    "        cap.append(period_rtrn(assets, cap[-1], allocations, n_days, length))\n",
    "        \n",
    "        holder_date = split_date+ pd.DateOffset(days = n_days * 2) \n",
    "        \n",
    "        market_days = nyse.valid_days(start_date=split_date, end_date=holder_date)\n",
    " \n",
    "        twenty_business_days_later = market_days[n_days - 1]\n",
    "\n",
    "        split_date = np.datetime64(twenty_business_days_later.to_pydatetime())\n",
    "        \n",
    "        length -= 1\n",
    "        k += 1\n",
    "        \n",
    "    return cap"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T07:51:23.042072800Z",
     "start_time": "2023-12-04T07:51:23.011551300Z"
    }
   },
   "id": "bc29cf8d0c88904d"
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Months, Capital: $ 1000\n",
      ".\n",
      "..\n",
      "...\n",
      "....\n",
      "1 Months, Capital: $ 1013.8266\n",
      ".\n",
      "..\n",
      "...\n",
      "....\n",
      "2 Months, Capital: $ 1083.7187\n",
      ".\n",
      "..\n",
      "...\n",
      "....\n",
      "3 Months, Capital: $ 993.6563\n",
      ".\n",
      "..\n",
      "...\n",
      "....\n"
     ]
    }
   ],
   "source": [
    "portfolio_rtrns = backtest(basket, split_date, 1000, fake_split(construct(\"AAPL\", 20), split_date), 20)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T07:56:18.532791900Z",
     "start_time": "2023-12-04T07:51:25.508984400Z"
    }
   },
   "id": "9e4bdec322f96b28"
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000, 1013.8265761499466, 1083.7187264974887, 993.6563220312672, 945.3938308811338]\n"
     ]
    }
   ],
   "source": [
    "print(portfolio_rtrns)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T08:02:23.240689300Z",
     "start_time": "2023-12-04T08:02:23.232114900Z"
    }
   },
   "id": "28100290b33c91ce"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "2273dee304533051"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
